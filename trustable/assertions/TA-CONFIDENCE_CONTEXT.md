---
level: 1.16.1
normative: false
---

**Guidance**

To quantify confidence, either a subjective assessment or a statistical argument must be presented for each statement and then systematically and repeatably aggregated to assess whether the final deliverable is fit for purpose.

To improve the accuracy of confidence evaluations in reflecting reality, the following steps are necessary:

- Breaking down high-level claims into smaller, recursive requests.
- Providing automated evaluations whenever possible, and using subjective assessments from appropriate parties when automation is not feasible.
- Aggregating confidence scores from evidence nodes.
- Continuously adjusting previous confidence measures based on new evidence, using previously established confidence values.

As subjective assessments are progressively replaced with statistical arguments and past confidence evaluations are refined against new evidence, the accuracy of evaluations improves over time. When project circumstances inevitably change, existing statements are repurposed, with their associated confidence scores eventually offering insights into the systematic capability of the project to deliver according to set objectives. This process should itself be analysed to determine the maturity of any given confidence score. A suitable meta-analysis can assess long-term trends in score sourcing, score accumulation, and weighting mechanisms.

**Evidence**

- Confidence scores from other TA items

**Confidence scoring**

Confidence scoring for TA-CONFIDENCE is based on quality of the confidence
scores given to Statements

**Checklist**

- What is the algorithm for combining/comparing the scores?
- How confident are we that this algorithm is fit for purpose?
- What are the trends for each score?
- How well do our scores correlate with external feedback signals?
